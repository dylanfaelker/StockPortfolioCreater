{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_financial as npf\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import concurrent.futures as cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Strategy for filtering the stocks within the csv file:\n",
    " * Given a dataframe of tickers, this function first creates a new empty dataframe including the name of the ticker, the price of it, beta, standard deviation, the market cap and its returns\n",
    " * In the filtering stages\n",
    "     - It first checks if there are any duplicates\n",
    "     - It then checks to confirm the the stock is traded on US markets\n",
    "     - Finally it confirms the daily volume from Jul 2 2021 to October 22 2021 is at least 10000\n",
    " * In order to get the beta calculations, we get the necessary data needed of the market (S&P500) which was obtained ouside of the function and stored in a dataframe\n",
    " * Then, with the help of threading, a for loop that goes through the entirety of the received dataframe and it ...\n",
    "  - Gets the yfinance data for each stock in the dataframe \n",
    "  - Calculates the price, beta, standard deviation and returns to a single row dataframe and adds said dataframe to the main dataframe that was created at the beginning of the function\n",
    " * It then returns the final dataframe after escaping the threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes in a dataframe of tickers and filters out ones that are duplicates, not traded in the US \n",
    "#  or have an average volume less than 10000 for Jul 2 to Oct 22 2021. It produces a dataframe\n",
    "#  with all valid tickers and finance data to go with them.\n",
    "\n",
    "def filtering(Tickers):\n",
    "    \n",
    "    #Creates a new dataframe to store valid tickers and their financial data\n",
    "    Valid_Tickers =  pd.DataFrame({'Tickers': [],\n",
    "                                   'Price': [],\n",
    "                                  'Beta': [],\n",
    "                                  'STD': [],\n",
    "                                  'Returns': []})\n",
    "    \n",
    "    #makes sure there are no duplicates\n",
    "    for index in range(len(Tickers.index)):\n",
    "        if Tickers.iloc[index,0] in Tickers.iloc[index+1:]:\n",
    "            Tickers.drop([index])\n",
    "            \n",
    "    number = 0;\n",
    "    \n",
    "    #Threading\n",
    "    with cf.ThreadPoolExecutor() as executor:\n",
    "        \n",
    "        #creates a thread for each Ticker to gets it's history data\n",
    "        datarow = [executor.submit(filtering_thread, Tickers.iloc[index,0]) for index in range(len(Tickers.index))]\n",
    "        \n",
    "        #Adds each ticker's data to the dataframe\n",
    "        for row in cf.as_completed(datarow):\n",
    "            Valid_Tickers = Valid_Tickers.append(row.result())\n",
    "            number+=1\n",
    "            print(number)\n",
    "    \n",
    "    #Formats the data\n",
    "    Valid_Tickers.reset_index(inplace=True)\n",
    "    Valid_Tickers = Valid_Tickers[['Tickers', 'Price', 'Beta', 'STD', 'Returns']]\n",
    "    \n",
    "    #returns the dataframe with all the data\n",
    "    return (Valid_Tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes in a Ticker, filters it to ensure it is traded in the US and as enough volume.\n",
    "#  It then grabs the finacial data and returns a dataframe with a single row \n",
    "#  containing the ticker and the financial data\n",
    "\n",
    "def filtering_thread(Ticker):\n",
    "    \n",
    "    #Gets data for filtering\n",
    "    stock = yf.Ticker(Ticker)\n",
    "    stock_hist = stock.history(start=data_start, end=data_end, interval='1d')\n",
    "    \n",
    "    #gets the data for the last 3 months to check volume\n",
    "    volume_hist = stock_hist.iloc[-66:-1]\n",
    "    \n",
    "    #grabs stock info\n",
    "    info = stock.info\n",
    "\n",
    "    #Checks if stock is traded in the US\n",
    "    if 'market' in info and info['market'] == 'us_market' and not(volume_hist.empty):\n",
    "\n",
    "        #Checks if the daily volume is at least 10000\n",
    "        total_sum = stock_hist.Volume.sum(axis=0)\n",
    "        average = total_sum/(len (stock_hist))\n",
    "        if average >= 10000:\n",
    "\n",
    "            #Gets monthly histru for that time\n",
    "            monthly_hist=stock_hist.resample('MS').first()\n",
    "            prices = pd.DataFrame(monthly_hist['Close'])\n",
    "            monthly_returns = prices.pct_change()\n",
    "\n",
    "            #creates a dataframe for just the daily closing price \n",
    "            #  and another one for just daily returns\n",
    "            daily_price = pd.DataFrame(stock_hist['Close'])\n",
    "            daily_returns = daily_price.pct_change()\n",
    "\n",
    "            ####### Price #############\n",
    "\n",
    "            #Closing price for the last day availible (Nov 26 when run for competition)\n",
    "            price = stock_hist['Close'].iloc[-1]\n",
    "\n",
    "            ######## Beta #############\n",
    "\n",
    "            #Adds markets daily returns to the dataframe\n",
    "            daily_returns['Market'] = daily_market_returns['Close']\n",
    "\n",
    "            #Calculates beta\n",
    "            beta = daily_returns.cov() / daily_market_returns['Close'].var()\n",
    "\n",
    "            ######### STD #############\n",
    "\n",
    "            #calculated standard deviation\n",
    "            std = prices.pct_change().std()\n",
    "\n",
    "            #returns a dataframe with the tickers price, beta, std and a dataframe for it's returns\n",
    "            return pd.DataFrame({'Tickers': [Ticker],\n",
    "                               'Price': [price],\n",
    "                              'Beta': [beta.iat[1,0]],\n",
    "                              'STD': [std.Close],\n",
    "                              'Returns': [stock_hist['Close'].pct_change()]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Strategy for choosing the 1 sinlge riskiest stock\n",
    "\n",
    "* We gathered 3 stocks with the highest standard deviation on monthly returns to be the riskiest stocks. From these 3 stocks, we calculated the riskiest stock to be the one with the highest beta value.\n",
    "* Stocks with high standard deviation are considered risky as they are stocks with high volatility and great fluctuations with prices. In addition, stocks with high beta values are considered risky as they are more volatile when compared to the overall market. \n",
    "* Since standard deviation and beta are both measures of riskiness and we wanted to take both into consideration. We did so by narrowing down the stocks to those that are risky in terms of standard deviation and then the one that is riskiest in terms of beta\n",
    "* There are some limitations with this approach which we will touch on later that has to do with as unlucky set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes in a set of tickers and determines which ticker is the \n",
    "#  rickiest based on standard deviation and beta\n",
    "\n",
    "def riskiest (Tickers):\n",
    "    \n",
    "    # gets the 3 tickers with the highest standard deviation\n",
    "    largest3_std = Tickers.nlargest(3, ['STD'])\n",
    "    \n",
    "    # gets the highest beta value from the 3 tickers with the highest standard deviation\n",
    "    largest_beta = largest3_std.nlargest(1, ['Beta'])\n",
    "    \n",
    "    #returns the riskiest stock\n",
    "    return (largest_beta)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Discussion for our decision in choosing the stocks to include\n",
    "* After determining the riskest stock among all the qualified stocks in the given csv file, we now want to decide what other stocks we should include in our portfolio in order to achieve a high-risk level. We will use the characteristics of this riskest stock as our guide to choosing the remaining stocks. The key idea of our approach is to **minimize the effect of diversification**.\n",
    "* The following steps explain in detail our code and why we do this. \n",
    ">1. Firstly, we filter out 20 stocks that are most correlated to the riskest stock determined before. The purpose is to make sure the stocks in our final portfolio are highly correlated, and thus less diversified. As the riskest stock is expected to fluctuate a lot, we expect the rest of the stocks in the portfolio to change in a similar way, so that the total fluctuation is larger. \n",
    ">2. Now, we filter out the 9 riskest stocks from the list of 20 correlated stocks obtained in the previous step. To determine their risk level, we mainly look at their standard deviations, as this metric measures fluctuations. We only pick the 9 riskest stocks, because the minimum number of stocks we need to have is 10. To minimize the effect of diversification, we want minimum stocks in our portfolio. \n",
    "\n",
    "* Discussions:\n",
    "    * After designing step 1, we recognized that solely looking at the correlation will not guarantee a high-risk level. To illustrate, consider the case where stock A and stock B have a similar change pattern, say correlation is 0.9, yet, while stock A fluctuates dramatically, the extent to which stock B prices fluctuate can be minimal. Then, adding stock B to the stock A portfolio can decrease the risk level if the flucatuations in stock B can't make up for the risk lost due to diversification. Therefore, we will also consider the risk level of each individual stock, which is step 2. \n",
    "    * Finally, we will include a total of 10 stocks in our portfolio  (i.e., 1 riskest stock we determine before + 9 stocks picked in this process), with the property that, every stock is highly correlated to one single stock (the riskest one) and in a high-risk level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tickers is the dataframe with all the stock and their data (beta, std, mcap, returns)\n",
    "#Corr is the stock that is being used to get correlation (riskiest). It should be a dataframe\n",
    "#  similar to Tickers but with only one row.\n",
    "#  It produces a dataframe of 10 stocks and their data which will be the stock in Corr \n",
    "#  and 9 other most correlated and risky stocks.\n",
    "\n",
    "def other_9(Tickers, Corr):\n",
    "    \n",
    "    #creates a dataframe to store values\n",
    "    Correlation =  pd.DataFrame({'Tickers': [],\n",
    "                                 'Price': [],\n",
    "                                 'Beta': [],\n",
    "                                 'STD': [],\n",
    "                                 'Returns': [],\n",
    "                                 'Corr': []})\n",
    "    \n",
    "    #gets returns for the riskiest stock\n",
    "    returns = Corr.iloc[0,4]\n",
    "    \n",
    "    #loops through the tickers\n",
    "    for index in range(len(Tickers.index)):\n",
    "        \n",
    "        #makes sure it doesn't get correaltion with itself\n",
    "        if not(Tickers.iloc[index, 0] == Corr.iloc[0,0]):\n",
    "            \n",
    "            #gets the returns for new stock being checked for correlation\n",
    "            stock_returns = Tickers.iloc[index, 4]\n",
    "            \n",
    "            #combines the monthly returns for the risky and other stock in one dataframe\n",
    "            returns = pd.concat([returns, stock_returns], join='inner', axis=1)\n",
    "            returns.columns = ['Risky', 'Other']\n",
    "            \n",
    "            #adds correlation data to the main dataframe with all stocks and data\n",
    "            Correlation = Correlation.append(pd.DataFrame({'Tickers': [Tickers.iloc[index,0]],\n",
    "                                                            'Price': [Tickers.iloc[index,1]],\n",
    "                                                            'Beta': [Tickers.iloc[index,2]],\n",
    "                                                            'STD': [Tickers.iloc[index,3]],\n",
    "                                                            'Returns': [Tickers.iloc[index,4]],\n",
    "                                                            'Corr': [returns.corr().iat[0,1]]}))\n",
    "            \n",
    "            #removes the returns for the other stock\n",
    "            returns = returns[['Risky']]\n",
    "            \n",
    "    #Gets the top 20 most correlated stocks\n",
    "    most_correlated_20 = Correlation.nlargest(20, 'Corr')\n",
    "    \n",
    "    #Gets the top 9 riskiest stocks from the most correlated\n",
    "    risky_9 = most_correlated_20.nlargest(9, 'STD')\n",
    "    \n",
    "    #Combines the dataframe into one final stock with the risky stock\n",
    "    final = Corr.append(risky_9)\n",
    "    \n",
    "    #formatting\n",
    "    final.reset_index(inplace=True)\n",
    "    final = final[['Tickers', 'Price', 'Beta', 'STD', 'Returns', 'Corr']]\n",
    "    \n",
    "    #returns top 10 stocks\n",
    "    return(final)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Strategy for deciding the weights\n",
    "* Firstly, we assign 35% to the riskest stock. 35% is the maximum we can give to one single stock. We choose to do so because we aim to achieve a high risk. By distributing the most weight to the riskest stock, we want our final portfolio to behave more like the riskiest and thus fluctuate more. \n",
    "* Secondly, we assign 5% to all of the rest 9 stocks. 5% is the minimum we can give to every single stock. \n",
    "* Finally, we distribute the other 20% to only the second third and fourth stocks in our list in the way that the new weight distribution gives us the highest total standard deviation. To do this, we use 2 for loops to interate between between all the distributions, calculate the standard deviation of the entire portfolio and choose the combination that maximises standard deviation. We don't give more weightings to the last 6 stocks because they are less risky and doing so will create more diversification.\n",
    "* One limitation of this program is the fact that we can't iterate through all the ways to distribute the final $\\$20000 between all 9 other stocks. Doing so creates exponetial blow up and trying to do it for all 9 would take 66 years. The most we can iterate though ina resonable time is 3 which takes about 0.6 seconds. Once you start doing 4, it takes a couple minutes and anything more than that will take too long. For this reason, we are only iterating between distribution for 3 stocks. Those 3 stocks are the 3 with the highest standard deviation since they are the most likely to increase riskiness level by being given more weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes in a list of 10 tickers and produces weightings for a portfolio to maximise risk level\n",
    "\n",
    "def weightings(Tickers, availibleCash): \n",
    "    \n",
    "    #Creates list of tickers in the order of risk level \n",
    "    Tickers10 = Tickers['Tickers'].iloc[0:10].tolist()\n",
    "\n",
    "    #Creates the initial weight distribution, which is not set in stone\n",
    "    weights = [0.35, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
    "\n",
    "    #Creates a list to store Tickers except the second to forth most risky stocks\n",
    "    RestTickers = Tickers10[4:10]\n",
    "    RestTickers.insert(0, Tickers10[0])\n",
    "\n",
    "    #Gets data from yf for those tickers, and stores closing pirces in a dataframe\n",
    "    data = yf.download(RestTickers,start= \"2010-01-01\", end= datetime.today(),period='1d')\n",
    "    restprices = pd.DataFrame(data['Close'])\n",
    "    restprices = restprices.dropna()\n",
    "    restprices.columns = RestTickers\n",
    "\n",
    "    #Creates a list for the weights for each of the stocks in RestTickers (these will not changed) \n",
    "    restweights = weights[4:10]\n",
    "    restweights.insert(0, weights[0])\n",
    "\n",
    "    #Creates a column in the prices dataframe for the unchanging tickers with for the portfolio value overtime\n",
    "    restprices['Portfolio_Value'] = 0\n",
    "    restprices['Portfolio_Value'].iloc[0]=100000*0.65 #sets first row to be the total value of these 7 stocks which is $65000\n",
    "\n",
    "    #Calculates the value of each of the 7 stocks based on each of their weights\n",
    "    for ticker in RestTickers:\n",
    "        restprices[ticker+'_SharesPurchased']= 0\n",
    "        restprices[ticker+'_SharesPurchased'].iloc[0] = 100000 * restweights[RestTickers.index(ticker)] / restprices[ticker][0]\n",
    "\n",
    "    #Calculates the total portfolio value over time\n",
    "    for x in range(1,len(restprices.index)):\n",
    "        for ticker in RestTickers:\n",
    "            restprices['Portfolio_Value'].iloc[x] += restprices[ticker+'_SharesPurchased'].iloc[0] * restprices[ticker].iloc[x]\n",
    "\n",
    "    #Creates a list of the Tickers of the second, third and forth most risky stocks\n",
    "    Two_Three_Four_Tickers = [Tickers10[1], Tickers10[2], Tickers10[3]]\n",
    "\n",
    "    #Gets closing prices from yahoo finance for each of these stocks\n",
    "    data1 = yf.download(Two_Three_Four_Tickers,start= \"2010-01-01\", end= datetime.today(),period='1d')\n",
    "    prices = pd.DataFrame(data1['Close'])\n",
    "    prices = prices.dropna()\n",
    "    prices.columns = Two_Three_Four_Tickers\n",
    "\n",
    "    #Sets the starting value of each stock to be $100000\n",
    "    #We will later find a portion of the values when determining the riskest weightings for these stocks\n",
    "    for ticker in Two_Three_Four_Tickers:\n",
    "        prices[ticker+'_SharesPurchased']= 0\n",
    "        prices[ticker+'_Value']=0\n",
    "        prices[ticker+'_Value'].iloc[0]=100000\n",
    "        prices[ticker+'_SharesPurchased'].iloc[0] =  100000 / prices[ticker][0]\n",
    "        \n",
    "    #Calculates value over time for each stock\n",
    "    for x in range(1,len(prices.index)):\n",
    "        for ticker in Two_Three_Four_Tickers:\n",
    "            prices[ticker+'_Value'].iloc[x] = prices[ticker+'_SharesPurchased'].iloc[0] * prices[ticker][x]\n",
    "\n",
    "    #Combines the two dataframes into one that will contain all value and pricing data overtime\n",
    "    combined = pd.concat([restprices,prices],join='inner', axis=1)\n",
    "\n",
    "    #Creates a new dataframe to store standard deviations and their weightings\n",
    "    totalstd = pd.DataFrame(index=range(0, 21))\n",
    "    totalstd['Standard_Deviation'] = ''\n",
    "    totalstd['Weight_2'] = ''\n",
    "\n",
    "    #Creates a dataframe to store the value of each of the stocks that have a dynamic weightings\n",
    "    value_2= combined[Tickers10[1]+\"_Value\"]\n",
    "    value_3= combined[Tickers10[2]+\"_Value\"]\n",
    "    value_4= combined[Tickers10[3]+\"_Value\"]\n",
    "\n",
    "    #Loops through all possible ways to ditribute $20000 in portions of $1000\n",
    "    for x in totalstd.index:\n",
    "        \n",
    "        #creates a dataframe to store the weightings of the first stock and the standard deviation that goes with it\n",
    "        weight23 = pd.DataFrame(index=range(0,21-x))\n",
    "        weight23['Standard_Deviation'] = ''\n",
    "        weight23['weight1'] = x\n",
    "\n",
    "        #loops through all ways to distribute the remaining cash not used by the first stock to the second and third stock\n",
    "        for y in range(0,21-x):\n",
    "            #Calculates the value and standard deviation of the portfolio for the current weightings\n",
    "            total_values= combined.Portfolio_Value + (value_2 *(5+x)/100) + (value_3 * (5+y)/100) + (value_4 * (25-x-y)/100)\n",
    "            portfolio = pd.DataFrame(total_values)\n",
    "            returns = pd.DataFrame(portfolio.pct_change())\n",
    "            \n",
    "            #Adds the standard deviation to a dataframe\n",
    "            weight23['Standard_Deviation'].iloc[y] = returns.std()[0]\n",
    "\n",
    "        #Calculates the biggest standard devaition of the dataframe\n",
    "        #  which changes how cash was distributed betweeen the second and third stock\n",
    "        std1 = weight23['Standard_Deviation'].max()\n",
    "        \n",
    "        #Adds the max standard deviation and it's weights to another dataframe which \n",
    "        #  holds standard deviations for different amounts cash in the first stock\n",
    "        weight2 = weight23.index[weight23['Standard_Deviation']==std1].tolist().pop(0)\n",
    "        totalstd['Weight_2'].iloc[x] = weight2\n",
    "        totalstd['Standard_Deviation'] = std1\n",
    "\n",
    "    ## Determines the weighting for the stock with the biggest standard deviation\n",
    "    diff = totalstd[totalstd.Standard_Deviation == totalstd['Standard_Deviation'].max()].index[0]\n",
    "    \n",
    "    #Updates the weights in the original weights dataframe\n",
    "    weights[1]=(5 + diff)/100\n",
    "    weights[2]=(5 + totalstd.iloc[diff].Weight_2)/100\n",
    "    weights[3]=(25 - diff - totalstd.iloc[diff].Weight_2)/100  \n",
    "    \n",
    "    #Creates a final dataframe to output\n",
    "    FinalPortfolio = Tickers\n",
    "    \n",
    "    #adds weights to the dataframe\n",
    "    weights = pd.Series(weights)\n",
    "    FinalPortfolio['weights'] = weights\n",
    "    \n",
    "    #Calculates number of shares of bought of each stock\n",
    "    FinalPortfolio['Shares'] = (FinalPortfolio.weights * availibleCash) / FinalPortfolio.Price\n",
    "    \n",
    "    #Creates column for the value of each stock within the portfolio\n",
    "    FinalPortfolio['Value'] = FinalPortfolio.Price * FinalPortfolio.Shares\n",
    "    \n",
    "    #Creates a new colummn for the weights at the end of the dataframe with the numbers in terms of %\n",
    "    FinalPortfolio['Weight'] = FinalPortfolio.weights * 100\n",
    "    \n",
    "    #Formatting\n",
    "    FinalPortfolio = FinalPortfolio[['Tickers', 'Price', 'Shares', 'Value', 'Weight']]\n",
    "    FinalPortfolio.columns = ['Ticker', 'Price', 'Shares', 'Value', 'Weight']\n",
    "    \n",
    "    #returns a final portfolio with the purchasing data for the stock being baught\n",
    "    return(FinalPortfolio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Next Steps:\n",
    "    \n",
    "Our project evaluated the risk of stocks based on high standard deviation, high beta value and low diversification. However there are other factors that could have helped to determine the risk level of stocks. These include calculating R-squared (coefficient of determination), Value at Risk (VaR) and/or market capitalization. \n",
    "\n",
    "The coefficient of determination displays the percentage of a fund or security's movements based on movements in a benchmark index (for example the Standard & Poor's 500 index). This value helps determine how likely a stock would drop if it's benchmark index dropped. A stock with a high R squared value measured against the riskiest stock from the portfolio can be considered risky. Value at Risk provides a worst-case scenario analysis where it calculates the percent of loss based on a time period and confidence level. It measures the risk of loss for investments. A stock with a high VaR value would be risky as the probability of losing that investment is high. Market cap is the total value of a companyâ€™s stocks. It is calculated by multiplying the number of outstanding shares with the current price of each share. A company with a small market cap is deemed more risky than a company with a large market cap. This is because companies with small market caps tend to be young companies with more uncertainties and high volatility. Incorporating these additional risk factors into our project could provide a better portfolio of risky stocks as there would be more factors evaluated with each stock. \n",
    "\n",
    "When it comes to limitations to our project, the steps that we take to come up with the riskiest stocks may not produce the best results. For example, when we consider 20 stocks that are highly correlated with the riskiest stock, we would not consider stocks that are risky (according to the beta value and standard deviation) but not correlated with the riskiest stock. One way we can fix the issue is by dynamically setting a minimum value for the correlation coefficient and then gather the stocks that fit the requirement. In the case that there are less than 9 stocks, we would decrease the minimum value of the correlation coefficient until we reach the required number of stocks.\n",
    "\n",
    "Another limitation comes from the narrowing down of stocks using different values. There can be unlucky set of data where you do not even come close to ending up with a risky set of stocks. Take for example a set where stock A has a STD (standard devaition) of 20 and a beta of 1, stock B has a STD of 8 and a beta of 1.01, and stock C has a STD of 7 and a beta of 1.1. If these were the 3 stocks with the highest STD when trying to find the riskiest, clearly the top riskiest is stock A. But since we choose our riskiest stock off of beta from the top 3 STDs, our algorithm would produce stock C as the riskiest. If we were to fix this issue, we could create a points system that assigns certain amounts of points for each measure of risk. In the above scenario, stock A would get alot of points for it's STD and the other stocks wouldn't be able to get enough from beta to catch up. Perfecting the points values could take awhile but it would eliminate the possibility of the above situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dataframe with the inputted tickers and their current prices\n",
    "\n",
    "def grab_tickers(Tickers):\n",
    "    \n",
    "    #Creates a new dataframe to store tickers and there price\n",
    "    Valid_Tickers =  pd.DataFrame({'Ticker': [],\n",
    "                                   'Price': []})\n",
    "            \n",
    "    number = 0;\n",
    "    \n",
    "    #Threading\n",
    "    with cf.ThreadPoolExecutor() as executor:\n",
    "        \n",
    "        #creates a thread for each Ticker to gets it's history data\n",
    "        datarow = [executor.submit(price_thread, Tickers.iloc[index,0]) for index in range(len(Tickers.index))]\n",
    "        \n",
    "        #Adds each ticker's data to the dataframe\n",
    "        for row in cf.as_completed(datarow):\n",
    "            Valid_Tickers = Valid_Tickers.append(row.result())\n",
    "            number+=1\n",
    "            print(number)\n",
    "    \n",
    "    #Formats the data\n",
    "    Valid_Tickers.reset_index(inplace=True)\n",
    "    Valid_Tickers = Valid_Tickers[['Ticker', 'Price']]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #returns the dataframe with all the data\n",
    "    return (Valid_Tickers)\n",
    "\n",
    "# Takes in a Ticker and finds its most recent closing price\n",
    "\n",
    "def price_thread(Ticker):\n",
    "    \n",
    "    #Gets data for filtering\n",
    "    \n",
    "    today = datetime.today()\n",
    "    yesterday = today - timedelta(days=7)\n",
    "\n",
    "    stock = yf.Ticker(Ticker)\n",
    "    stock_hist = stock.history(start=yesterday, end=today, interval='1d')\n",
    "    close_today = stock_hist.iloc[-1, 3]\n",
    "    \n",
    "    return pd.DataFrame({'Ticker': [Ticker],\n",
    "                         'Price': [close_today]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines the current value of the portfolio\n",
    "\n",
    "def calc_funds():\n",
    "    \n",
    "    # Grabs the past data from the csv\n",
    "    past_data = pd.read_csv('Portfolio_History.csv')\n",
    "    \n",
    "    # Creates a dataframe of past tickers most recent share count sorted alphabetically by ticker\n",
    "    prev_shares = past_data.iloc[1: , :]\n",
    "    prev_shares = prev_shares.set_index('Ticker')\n",
    "    prev_shares.sort_index(inplace=True) \n",
    "    prev_shares.reset_index(inplace=True)\n",
    "    prev_shares = prev_shares.iloc[: , -1:]\n",
    "    \n",
    "    # Creates a dataframe of past tickers sorted alphabetically\n",
    "    prev_tickers = past_data.iloc[1: , 1:2]\n",
    "    prev_tickers = prev_tickers.set_index('Ticker')\n",
    "    prev_tickers.sort_index(inplace=True) \n",
    "    prev_tickers.reset_index(inplace=True)\n",
    "    prev_tickers = prev_tickers[['Ticker']]\n",
    "    \n",
    "    # Optimises the reading of ticker price\n",
    "    prev_tickers['Shares'] = prev_shares\n",
    "    prev_tickers = prev_tickers.dropna()\n",
    "    prev_tickers = prev_tickers.reset_index()\n",
    "    prev_tickers = prev_tickers[['Ticker']]\n",
    "    \n",
    "    prev_shares = prev_shares.dropna()\n",
    "    prev_shares = prev_shares.reset_index()\n",
    "    prev_shares = prev_shares.iloc[: , -1:]\n",
    "    \n",
    "    # Grabs the closing price for today and sorts alphabetically by ticker\n",
    "    prices = grab_tickers(prev_tickers)\n",
    "    prices = prices.set_index('Ticker')\n",
    "    prices.sort_index(inplace=True) \n",
    "    prices = prices.reset_index()\n",
    "    \n",
    "    # Merges data and calculates total value of portfolio\n",
    "    prev = prices\n",
    "    prev['Shares'] = prev_shares\n",
    "    prev['Value'] = prev.Price * prev.Shares\n",
    "    availible_funds = prev.Value.sum(axis=0)\n",
    "    \n",
    "    return(availible_funds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates the Portfolio_History.csv and saves yesterdays data in Yesterday_Portfolio_History.csv\n",
    "\n",
    "def rewriting_csv(FinalPortfolio):\n",
    "    \n",
    "    # Adding total value to the top of the table\n",
    "    # Shares will be the total value in dollars of the total row\n",
    "    today_value = FinalPortfolio.Value.sum(axis=0)\n",
    "    value_row = []\n",
    "    value_row.insert(0, {'Ticker': 'Total Value', 'Price': today_value, 'Shares': today_value, 'Value': today_value, 'Weight': 100.0})\n",
    "    today_data = pd.concat([pd.DataFrame(value_row), FinalPortfolio], ignore_index=True)\n",
    "\n",
    "    # reformatting columns\n",
    "    today_data = today_data[['Ticker', 'Shares']]\n",
    "    today = datetime.today()\n",
    "    today_data.columns = ['Ticker', today]\n",
    "\n",
    "    # Grabbing past data\n",
    "    past_data = pd.read_csv('Portfolio_History.csv')\n",
    "    \n",
    "    # Saving yesterdays data incase of error\n",
    "    saving = past_data.iloc[:,1:]\n",
    "    saving.to_csv('Yesterday_Portfolio_History.csv')\n",
    "    \n",
    "    # removing extra index column\n",
    "    past_data = past_data.iloc[: , 1:]\n",
    "    full_data = past_data.merge(today_data, left_on='Ticker', right_on='Ticker', how='outer')\n",
    "\n",
    "    # Ordering the shares alphabetically\n",
    "    reorder_tickets = full_data.iloc[1:]\n",
    "    reorder_tickets = reorder_tickets.set_index('Ticker')\n",
    "    reorder_tickets.sort_index(inplace=True) \n",
    "    reorder_tickets = reorder_tickets.reset_index()\n",
    "\n",
    "    # Adding ordered share data back to dataframe\n",
    "    full_data.iloc[1:] = reorder_tickets\n",
    "\n",
    "    # Updating csv file\n",
    "    full_data.to_csv('Portfolio_History.csv')\n",
    "    print(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp500():\n",
    "    try:\n",
    "        # Grabs the SP500 tickers from wikipedia\n",
    "        wikipedia = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "        sp500 = wikipedia[0]\n",
    "        sp500_symbols = pd.DataFrame(sp500['Symbol'].values.tolist())\n",
    "        sp500_symbols.columns = ['Tickers']\n",
    "\n",
    "        # Saves current SP500 tickers incase of future issues\n",
    "        sp500_symbols.to_csv('S&P500.csv')\n",
    "    except:\n",
    "        # Grabs yesterdays data instead\n",
    "        print('ERROR in reading current SP500 companies')\n",
    "        print('Using last successful read instead')\n",
    "        sp500_symbols = pd.read_csv('S&P500.csv')\n",
    "        sp500_symbols = sp500_symbols.iloc[:,1:]\n",
    "        sp500_symbols.columns = [['Tickers']]\n",
    "        \n",
    "    return(sp500_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers grabbed\n",
      "Date range set\n",
      "Market data grabbed\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "- BRK.B: No data found, symbol may be delisted\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "- BF.B: No data found for this date range, symbol may be delisted\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "Tickets filtered\n",
      "Riskiest chosen\n",
      "Riskiest 10 chosen\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "[*********************100%***********************]  7 of 7 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faelk\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  3 of 3 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faelk\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio weightings decided\n",
      "  Ticker       Price      Shares         Value  Weight\n",
      "0   ENPH  200.669998  169.817184  34077.214102    35.0\n",
      "1   ETSY   78.730003   61.833777   4868.173443     5.0\n",
      "2   TSLA  705.210022    6.903154   4868.173443     5.0\n",
      "3   PENN   28.510000  853.765942  24340.867216    25.0\n",
      "4   SEDG  291.529999   16.698705   4868.173443     5.0\n",
      "5   PAYC  286.140015   17.013256   4868.173443     5.0\n",
      "6   QRVO   94.980003   51.254720   4868.173443     5.0\n",
      "7   NVDA  162.250000   30.004151   4868.173443     5.0\n",
      "8   GNRC  221.910004   21.937602   4868.173443     5.0\n",
      "9   TRMB   59.430000   81.914411   4868.173443     5.0\n",
      "Total value is: $97363.46886394368\n",
      "Total weight is: 100.0%\n",
      "         Ticker        01:25.8       16:44.4  2022-06-23 18:53:21.370964\n",
      "0   Total Value  100000.000000  94871.395900                97363.468864\n",
      "1           APA     867.410194           NaN                         NaN\n",
      "2           BKR     164.041997           NaN                         NaN\n",
      "3           DVN      82.562746           NaN                         NaN\n",
      "4          ENPH            NaN    171.664115                  169.817184\n",
      "5          ETSY            NaN     64.024430                   61.833777\n",
      "6          GNRC            NaN     22.315330                   21.937602\n",
      "7           HAL     745.156500           NaN                         NaN\n",
      "8           HES      46.408020           NaN                         NaN\n",
      "9           LNC     102.417043           NaN                         NaN\n",
      "10          MRO     200.803216           NaN                         NaN\n",
      "11         MTCH            NaN     68.135157                         NaN\n",
      "12         NVDA            NaN     28.994924                   30.004151\n",
      "13          OKE      90.025206           NaN                         NaN\n",
      "14          OXY      86.400555           NaN                         NaN\n",
      "15         PAYC            NaN     17.364899                   17.013256\n",
      "16         PENN            NaN    836.903629                  853.765942\n",
      "17         QRVO            NaN     49.801258                   51.254720\n",
      "18         SEDG            NaN     16.917154                   16.698705\n",
      "19          SLB     128.799589           NaN                         NaN\n",
      "20         TRMB            NaN           NaN                   81.914411\n",
      "21         TSLA            NaN      6.697498                    6.903154\n",
      "File created\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Price</th>\n",
       "      <th>Shares</th>\n",
       "      <th>Value</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENPH</td>\n",
       "      <td>200.669998</td>\n",
       "      <td>169.817184</td>\n",
       "      <td>34077.214102</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ETSY</td>\n",
       "      <td>78.730003</td>\n",
       "      <td>61.833777</td>\n",
       "      <td>4868.173443</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>705.210022</td>\n",
       "      <td>6.903154</td>\n",
       "      <td>4868.173443</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PENN</td>\n",
       "      <td>28.510000</td>\n",
       "      <td>853.765942</td>\n",
       "      <td>24340.867216</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SEDG</td>\n",
       "      <td>291.529999</td>\n",
       "      <td>16.698705</td>\n",
       "      <td>4868.173443</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PAYC</td>\n",
       "      <td>286.140015</td>\n",
       "      <td>17.013256</td>\n",
       "      <td>4868.173443</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>QRVO</td>\n",
       "      <td>94.980003</td>\n",
       "      <td>51.254720</td>\n",
       "      <td>4868.173443</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>162.250000</td>\n",
       "      <td>30.004151</td>\n",
       "      <td>4868.173443</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GNRC</td>\n",
       "      <td>221.910004</td>\n",
       "      <td>21.937602</td>\n",
       "      <td>4868.173443</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TRMB</td>\n",
       "      <td>59.430000</td>\n",
       "      <td>81.914411</td>\n",
       "      <td>4868.173443</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticker       Price      Shares         Value  Weight\n",
       "0   ENPH  200.669998  169.817184  34077.214102    35.0\n",
       "1   ETSY   78.730003   61.833777   4868.173443     5.0\n",
       "2   TSLA  705.210022    6.903154   4868.173443     5.0\n",
       "3   PENN   28.510000  853.765942  24340.867216    25.0\n",
       "4   SEDG  291.529999   16.698705   4868.173443     5.0\n",
       "5   PAYC  286.140015   17.013256   4868.173443     5.0\n",
       "6   QRVO   94.980003   51.254720   4868.173443     5.0\n",
       "7   NVDA  162.250000   30.004151   4868.173443     5.0\n",
       "8   GNRC  221.910004   21.937602   4868.173443     5.0\n",
       "9   TRMB   59.430000   81.914411   4868.173443     5.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reads in the csv file \n",
    "Tickers = get_sp500()\n",
    "\n",
    "print(\"Tickers grabbed\")\n",
    "\n",
    "#Sets constants for the data being collected\n",
    "data_end = datetime.today()\n",
    "data_start = data_end - timedelta(days=3653) #10 years\n",
    "\n",
    "print(\"Date range set\")\n",
    "\n",
    "#Grabs market data for S&P500\n",
    "market_index = yf.Ticker('^GSPC')\n",
    "market_hist = market_index.history(start=data_start, end=data_end, interval='1d')\n",
    "market_hist = pd.DataFrame(market_hist['Close'])\n",
    "daily_market_returns = market_hist.pct_change()\n",
    "\n",
    "print(\"Market data grabbed\")\n",
    "\n",
    "#Filters the stocks and gets their data\n",
    "Tickers = filtering(Tickers)\n",
    "\n",
    "print(\"Tickets filtered\")\n",
    "\n",
    "#Determines the riskiest stock\n",
    "riskiest = riskiest(Tickers)\n",
    "\n",
    "print(\"Riskiest chosen\")\n",
    "\n",
    "#Chooses the other 9 stocks\n",
    "final10 = other_9(Tickers, riskiest)\n",
    "\n",
    "print(\"Riskiest 10 chosen\")\n",
    "\n",
    "#Gets the weighting for the 10 stocks\n",
    "FinalPortfolio = weightings(final10, calc_funds())\n",
    "\n",
    "print(\"Portfolio weightings decided\")\n",
    "print(FinalPortfolio)\n",
    "\n",
    "#Calculates the total weights and value to prove the porfolio is valid\n",
    "totalValue = FinalPortfolio.Value.sum(axis=0)\n",
    "totalWeight = FinalPortfolio.Weight.sum(axis=0)\n",
    "print('Total value is: $' + str(totalValue))\n",
    "print('Total weight is: ' + str(totalWeight) + '%')\n",
    "\n",
    "#Creates a dataframe for the .csv file and creates the csv\n",
    "rewriting_csv(FinalPortfolio)\n",
    "\n",
    "print(\"File created\")\n",
    "FinalPortfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
